{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75264527-6a3f-446e-b177-b45d3d0c22e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T23:25:30.169892Z",
     "iopub.status.busy": "2025-06-03T23:25:30.168843Z",
     "iopub.status.idle": "2025-06-03T23:25:46.377743Z",
     "shell.execute_reply": "2025-06-03T23:25:46.376311Z",
     "shell.execute_reply.started": "2025-06-03T23:25:30.169832Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:25:37.711008: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-03 23:25:37.711223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-03 23:25:37.838945: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-03 23:25:38.101448: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-03 23:25:41.147183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21608e1c-6303-40cf-a441-b7c27fdb5fea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T23:38:59.262629Z",
     "iopub.status.busy": "2025-06-03T23:38:59.262040Z",
     "iopub.status.idle": "2025-06-03T23:55:34.708020Z",
     "shell.execute_reply": "2025-06-03T23:55:34.706808Z",
     "shell.execute_reply.started": "2025-06-03T23:38:59.262579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0640a13eaa86448eafd25abd662eadf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66b9fa58cfe4ace8048376e84ec62ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebf68f73fd34e7abb3f20aab63f064e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a8eb21233148219e252617a3b901a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929a12abaf5943c989a12fab6f519548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a42221ed324c8dbbe53a6b3096ee28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10c8f3d53174956bab2c1a9adc97e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2221867ddffd423496122f84756ca92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.20.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/notebooks/wandb/run-20250603_234323-oct6ceiu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/utecds/huggingface/runs/oct6ceiu' target=\"_blank\">stellar-wildflower-1</a></strong> to <a href='https://wandb.ai/utecds/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/utecds/huggingface' target=\"_blank\">https://wandb.ai/utecds/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/utecds/huggingface/runs/oct6ceiu' target=\"_blank\">https://wandb.ai/utecds/huggingface/runs/oct6ceiu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2274' max='2274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2274/2274 12:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.556080</td>\n",
       "      <td>0.821452</td>\n",
       "      <td>0.807654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.399900</td>\n",
       "      <td>0.468874</td>\n",
       "      <td>0.852145</td>\n",
       "      <td>0.842183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.470041</td>\n",
       "      <td>0.874917</td>\n",
       "      <td>0.865042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Fine-tuning completado y modelo guardado en ./emotion_model\n",
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f3220588610>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f322e6e3b10, execution_count=5 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f322cbd50d0, raw_cell=\"# ================================================..\" store_history=True silent=False shell_futures=True cell_id=21608e1c-6303-40cf-a441-b7c27fdb5fea> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# finetune_emotion_es.py\n",
    "# Fine-tuning de “daveni/twitter-xlm-roberta-emotion-es”\n",
    "# sobre tu dataset de tweets en español con etiquetas emocionales\n",
    "# ============================================================\n",
    "\n",
    "# 1) ──────────────── LIBRERÍAS ──────────────────────────────\n",
    "from datasets   import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd, numpy as np, re, torch, pathlib, csv, os\n",
    "\n",
    "# 2) ──────────────── CARGA + LIMPIEZA DEL CSV ───────────────\n",
    "def detect_delimiter(path):\n",
    "    \"\"\"Detecta automáticamente ',', ';' o tab.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sample = \"\".join([f.readline() for _ in range(5)])\n",
    "    return csv.Sniffer().sniff(sample, delimiters=[\",\", \";\", \"\\t\"]).delimiter\n",
    "\n",
    "CSV_PATH = \"dataset.csv\"\n",
    "delimiter = detect_delimiter(CSV_PATH)\n",
    "\n",
    "def clean_tweet(text: str) -> str:\n",
    "    \"\"\"• Quita URLs, @menciones, hashtags y tokens HASHTAG/USER\n",
    "       • Colapsa espacios múltiples.\"\"\"\n",
    "    text = re.sub(r\"https?://\\S+\", \" \", text)            # URLs\n",
    "    text = re.sub(r\"@\\w+\", \" \", text)                    # @menciones reales\n",
    "    text = re.sub(r\"#\\w+\", \" \", text)                    # hashtags reales (#libros)\n",
    "    text = re.sub(r\"\\b(HASHTAG|USER)\\b\", \" \", text, flags=re.I)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, delimiter=delimiter)\n",
    "df = df.dropna(subset=[\"tweet\", \"label\"])\n",
    "df[\"tweet\"] = df[\"tweet\"].astype(str).apply(clean_tweet)\n",
    "\n",
    "# 3) ──────────────── ENCODE DE ETIQUETAS ───────────────────\n",
    "labels     = sorted(df[\"label\"].unique())           # p.e. ['anger','joy',...]\n",
    "label2id   = {lab: idx for idx, lab in enumerate(labels)}\n",
    "id2label   = {idx: lab for lab, idx in label2id.items()}\n",
    "df[\"label\"] = df[\"label\"].map(label2id)            \n",
    "\n",
    "# 4) ──────────────── SPLIT TRAIN / VALID ───────────────────\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "val_df   = df.drop(train_df.index)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "# 5) ──────────────── TOKENIZADOR + MODELO ──────────────────\n",
    "MODEL_NAME = \"daveni/twitter-xlm-roberta-emotion-es\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 6) ──────────────── TOKENIZACIÓN ──────────────────────────\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"tweet\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds   = val_ds.map(tokenize,   batched=True)\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "train_ds.set_format(type=\"torch\", columns=cols)\n",
    "val_ds.set_format(  type=\"torch\", columns=cols)\n",
    "\n",
    "# 7) ──────────────── ARGUMENTOS DE ENTRENAMIENTO ───────────\n",
    "training_args = TrainingArguments(\n",
    "    output_dir            = \"./emotion_model\",\n",
    "    evaluation_strategy   = \"epoch\",\n",
    "    save_strategy         = \"epoch\",\n",
    "    logging_strategy      = \"epoch\",\n",
    "    learning_rate         = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size  = 16,\n",
    "    num_train_epochs      = 3,\n",
    "    weight_decay          = 0.01,\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model = \"accuracy\",\n",
    ")\n",
    "\n",
    "# 8) ──────────────── MÉTRICAS PERSONALIZADAS ───────────────\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\":       f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "# 9) ──────────────── TRAINER Y FINE-TUNING ────────────────\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = train_ds,\n",
    "    eval_dataset    = val_ds,\n",
    "    tokenizer       = tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 🔽 Guarda la versión fine-tuneada\n",
    "trainer.save_model(\"./emotion_model\")\n",
    "tokenizer.save_pretrained(\"./emotion_model\")\n",
    "\n",
    "print(\"\\n✅ Fine-tuning completado y modelo guardado en ./emotion_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
